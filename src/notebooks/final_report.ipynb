{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b256319",
   "metadata": {},
   "source": [
    "# Income Prediction from Census Data\n",
    "\n",
    "## Summary\n",
    "This report presents an analysis of the 1994 [Census Income dataset](https://archive.ics.uci.edu/dataset/2/adult), predicting whether individuals in the US earn above or below USD 50,000 annually (USD 110,000 in 2025 money). The analysis encompasses data cleaning, exploratory analysis, feature engineering, model development, and evaluation, culminating in a comparison of Generalized Linear Model (GLM) and Gradient Boosting Machine (GBM) approaches.\n",
    "\n",
    "### Key Results:\n",
    "- Final Models: LGBM achieved the highest accuracy, with 87.4% cross-validated accuracy. However, GLM still achieved good accuracy, with 84.5%, and may be the best model of the two depending on the business case.\n",
    "\n",
    "- Top 5 Predictive Features (LGBM):\n",
    "    - Capital Net: Investment income indicator\n",
    "    - Age: Life-cycle earnings pattern\n",
    "    - Hours per Week: Full-time vs part-time distinction\n",
    "    - Education: Human capital investment\n",
    "    - Relationship = Married: Household income dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47391d14",
   "metadata": {},
   "source": [
    "## Motivation for the Prediction Task\n",
    "\n",
    "Accurately predicting individual income levels has significant practical applications across multiple sectors. For example:\n",
    "\n",
    "**Government & Public Policy:**\n",
    "- **Welfare Eligibility Screening:** In countries lacking centralized income tracking (e.g., UK), predictive models can help identify individuals likely eligible for means-tested benefits (Winter Fuel Payment, Universal Credit) without intrusive income verification.\n",
    "\n",
    "**Non-Profit Sector:**\n",
    "- **Donor Prospecting:** Charities often use crude proxies (e.g., age, postcode) for wealth assessment during door-to-door fundraising. Data-driven income prediction can improve targeting efficiency, reducing wasted outreach while respecting privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb81bd",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis\n",
    "\n",
    "### Dataset Overview\n",
    "The dataset comes from the UCI Machine Learning Repository. The prediction task is to determine whether an individual's income exceeds USD 50,000 per year.\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Samples: 48,842 individuals\n",
    "- Original Features: 15 variables\n",
    "- Target Variable: Binary income classification (<=50K, >50K)\n",
    "- Source: US Census Bureau, 1994\n",
    "\n",
    "**Distribution**:\n",
    "\n",
    "Numerical distributions vary significantly. While 'Age' shows a smooth natural spread, 'Hours-per-week' is heavily concentrated (see visualization). Recognizing these distinct patterns, natural variance vs. structural concentration, is critical for accurate modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from income_predict_d100_d400.robust_paths import DATA_DIR\n",
    "from income_predict_d100_d400.final_report import plots\n",
    "\n",
    "parquet_path = DATA_DIR / \"census_income.parquet\"\n",
    "\n",
    "df_raw = pl.read_parquet(parquet_path)\n",
    "\n",
    "plots.distribution_variety(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b3a0f",
   "metadata": {},
   "source": [
    "### Initial Data Quality Assessment\n",
    "\n",
    "There were a number of minor data quality issues with the dataset, many of which could be resolved by simple transformations of the dataset. Below I visualise some of the data quality problems that needed to be dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.visualize_missing_data(df_raw)\n",
    "plots.binary_column_issue(df_raw, 'income', expected_values=['<=50K', '>50K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad753f",
   "metadata": {},
   "source": [
    "#### Cleaning Pipeline\n",
    "\n",
    "Cleaning steps taken:\n",
    "- Add unique ID column (there were 29 duplicates, this isn't unexpected as some people will have the same demographics. Adding unique ID allows us to tell these individuals apart)\n",
    "- Basic column name cleaning\n",
    "- Remove unnecessary columns\n",
    "- Clean target feature\n",
    "- Binarize income data (1 for '>50K', 0 for '<=50K') and rename to 'high_income'\n",
    "- Binarize sex and rename to is_female\n",
    "- Binarize race into two columns, is_white and is_black\n",
    "- Replace '?' values with pandas NaN\n",
    "- Trim whitespace from all string values\n",
    "- Order and transform education. Change datatype from str to int (e.g. 'Preschool' = 1 < '1st-4th' = 2 < ... < 'Doctorate' = 16)\n",
    "- Transform capital-gain and capital-loss into a single column, capital_net\n",
    "- Simplify relationship column. Bundle 'Wife' and 'Husband' values into one, 'Married'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from income_predict_d100_d400 import cleaning\n",
    "\n",
    "df_clean = cleaning.full_clean(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce7dff",
   "metadata": {},
   "source": [
    "## Feature Selection & Engineering\n",
    "\n",
    "### Feature Engineering Approach\n",
    "\n",
    "To ensure robust performance across both models, I implemented a shared preprocessing pipeline, with specific enhancements driven by the GLM's sensitivity:\n",
    "\n",
    "1. **Shared Preprocessing (Both Models)**:\n",
    "\n",
    "    - **Imputation**: I prevented leakage by imputing missing values within the cross-validation pipeline (median for numeric, \"missing\" category for categorical).\n",
    "\n",
    "    - **Encoding**: Nominal variables were One-Hot Encoded. While LGBM can handle categories natively, OHE ensures a consistent feature space for direct model comparison.\n",
    "\n",
    "2. **GLM-Specific Enhancements**:\n",
    "\n",
    "    - **Scale & Skew**: Unlike the tree-based LGBM which is invariant to monotonic transformations, the GLM requires strictly scaled data. I applied our custom SignedLogTransformer followed by StandardScaler to all numeric features to ensure efficient convergence.\n",
    "\n",
    "    - **Interactions**: I manually engineered an age_x_education interaction term. The LGBM learns these non-linearities automatically, but the GLM requires explicit features to capture life-cycle effects.\n",
    "\n",
    "### Feature Selection \n",
    "Selection was driven by EDA findings and economic intuition:\n",
    "\n",
    "- **Numeric**: age, hours_per_week, and education (ordinally encoded to preserve rank).\n",
    "\n",
    "- **GLM Interaction**: I manually engineered an age_x_education interaction term to capture changing returns to education over the life-cycle—a non-linear relationship the LGBM learns implicitly but the GLM requires explicitly.\n",
    "\n",
    "- **Demographics**: sex and race were binarized (is_female, is_white, is_black).\n",
    "\n",
    "- **Categorical**: occupation, work_class, and relationship were retained for their strong correlation with income.\n",
    "\n",
    "- **Exclusions**: I removed fnlwgt (non-predictive), education-num (redundant), and marital_status (to avoid multicollinearity with relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027349ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.correlation_compare(df_clean)\n",
    "plots.occupation_correlation(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523877e9",
   "metadata": {},
   "source": [
    "### Train-Test Split Strategy\n",
    "\n",
    "**Split Methodology**:\n",
    "- Method: Hash-based splitting on unique_id (80% train, 20% test)\n",
    "- Why: Hash function ensures reproducible split across runs. This ensures no data leakage\n",
    "\n",
    "**Cross-Validation for Hyperparameter Tuning**:\n",
    "- Method: 5-fold Stratified K-Fold on training set only\n",
    "- Why: Stratification preserves class balance in each fold; multiple folds provide robust performance estimates and reduce overfitting to a single validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3dc7c",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Two complementary approaches were selected:\n",
    "\n",
    "1. Generalized Linear Model (GLM)\n",
    "    - For GLM implementation, I selected the SGDClassifier with log-loss. This allowed for direct tuning of the alpha and l1_ratio regularization parameters\n",
    "2. LightGBM (LGBM)\n",
    "    - For the LGBM implementation, I utilized the standard lightgbm.LGBMClassifier. \n",
    "\n",
    "**Why GLM?**:\n",
    "\n",
    "- Interpretability: Coefficients show linear relationship between each feature and the target\n",
    "- Baseline: Establishes performance floor. Simpler models preferred if comparable\n",
    "- Fast Training: Scales well to large datasets\n",
    "- Assumptions: Appropriate if relationships are approximately linear\n",
    "\n",
    "**Why LGBM?**:\n",
    "\n",
    "- Non-linear Relationships: Captures complex interactions between features\n",
    "- Feature Interactions: Automatically learns combinations without manual engineering\n",
    "- Robust: Handles missing values, outliers, and mixed feature types well\n",
    "- Performance: Generally highly accurate, in comparisson to GLM and other boosting models\n",
    "- Feature Importance: Provides interpretable feature rankings\n",
    "\n",
    "### Evaluation Approach\n",
    "\n",
    "#### Metric Selection\n",
    "\n",
    "Metric Selection and Rationale:\n",
    "\n",
    "| Metric | Why Important | Interpretation |\n",
    "|--------|---------------|----------------|\n",
    "| **Accuracy** | Overall correctness | % of correct predictions |\n",
    "| **Precision** | Minimize false positives | Of predicted high-income, % actually high-income |\n",
    "| **Recall** | Minimize false negatives | Of actual high-income, % correctly identified |\n",
    "| **F1-Score** | Balance precision/recall | Harmonic mean; useful for imbalanced data |\n",
    "| **ROC-AUC** | Ranking quality | Probability of ranking random positive > negative |\n",
    "| **Log Loss** | Probability calibration | Penalizes confident wrong predictions |\n",
    "\n",
    "\n",
    "**Primary Metric/s**: It depends on the exact business question. For example, charity donor prospecting will likely want a model with high precision and Log Loss, so that they can target people that are most likely to have high income.\n",
    "\n",
    "#### Evaluation Strategy\n",
    "\n",
    "- Train Set: Used for model training and hyperparameter tuning (with cross-validation)\n",
    "- Test Set: Held out for final evaluation only (no model selection on test set)\n",
    "- Cross-Validation: 5-fold stratified CV during hyperparameter tuning\n",
    "    - Ensures robust parameter selection\n",
    "    - Reduces overfitting to single validation split\n",
    "    - Maintains class balance in each fold\n",
    "- Preventing Overfitting:\n",
    "    - Separate train/test split (no test set leakage)\n",
    "    - Cross-validation for hyperparameter selection\n",
    "    - Regularisation for GLM\n",
    "    - Early stopping from LGBM\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "#### GLM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `alpha` | 10^-4 to 10^-1 (log-uniform) | Regularization strength (higher = more regularization) | 0.0001 | **0.000861** |\n",
    "| `l1_ratio` | 0 to 1 (uniform) | Balance between L1 (Lasso) and L2 (Ridge) penalty | 0.15 | **0.520** |\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (more efficient than grid search for continuous parameters)\n",
    "- **Iterations:** 50 random combinations sampled\n",
    "- **Cross-Validation:** 5-fold stratified ensures robust evaluation\n",
    "- **Search Strategy:** Log-uniform for `alpha` (covers multiple orders of magnitude)\n",
    "\n",
    "#### LGBM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `learning_rate` | 0.01 to 0.2 (log-uniform) | Controls step size in gradient descent | 0.1 | **0.158** |\n",
    "| `n_estimators` | 50 to 200 (integer) | Number of boosting rounds (trees) | 100 | **89** |\n",
    "| `num_leaves` | 10 to 60 (integer) | Maximum leaves per tree (complexity) | 31 | **30** |\n",
    "| `min_child_weight` | 0.0001 to 0.002 (log-uniform) | Minimum sum of instance weight in leaf (regularization) | 0.001 | **0.00013** |\n",
    "\n",
    "**Note on Early Stopping**: Instead of utilizing early stopping, I included the number of boosting rounds (n_estimators) directly in the hyperparameter search space. This allowed the optimal number of trees to be selected via Stratified K-Fold Cross-Validation, ensuring the model complexity was validated robustly across multiple data splits rather than a single hold-out set.\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (efficient for large search space)\n",
    "- **Iterations:** 20 combinations (LGBM slower to train than GLM)\n",
    "- **Cross-Validation:** 5-fold stratified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f095fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077329cc",
   "metadata": {},
   "source": [
    "**Performance Interpretation:**\n",
    "\n",
    "- LGBM outperforms GLM across nearly all metrics.\n",
    "- The most striking result is GLM's bias reduction after tuning, 80% improvement. This means the baseline GLM was systematically over-predicting and tuning corrected this calibration issue\n",
    "- LGBM showed modest gains from tuning, with small improvements across metrics. This indicates the baseline LGBM hyperparameters were already near-optimal for this dataset.\n",
    "\n",
    "I shall only focus on the tuned models moving forward.\n",
    "\n",
    "**Classification Performance Analysis**\n",
    "\n",
    "Classification performance matters because it reveals not just overall accuracy, but where the model is accurate, i.e. false positives vs false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabedee",
   "metadata": {},
   "source": [
    "**Confusion Matrix Insights**:\n",
    "\n",
    "LGBM beats GLM across all metrics in the classification analysis.\n",
    "\n",
    "Given LGBM is better at avoiding false positives and false negatives, there is no need to consider which is more important. However, if one model was better at false positives and the other better at false negatives, I would need to consider the business implications to determine which model would be preferred.\n",
    "\n",
    "**Feature Importance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d67cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.feature_importance_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663924a5",
   "metadata": {},
   "source": [
    "**Feature Importance Insights**\n",
    "\n",
    "Interestingly, the Feature Importance chart shows that the GLM model and LGBM model can rate particular features very similarly, as well as very differently. This highlights the  differences in how these models learn patterns from data. GLM relies on linear, individual correlations, whereas LGBM can captures non-linear interactions and handles feature redundancy. For example, GLM ranks 'Own-child' highly compared to LGBM, this suggests that there is a relationship with income, its possible to extract much of the signal through other features like age and hours worked.\n",
    "\n",
    "**Partial Dependence Analysis**\n",
    "\n",
    "Below are three Partial Dependence charts provinding insights into predicting high income, as well as understanding how GLM and LGBM models work generally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "from income_predict_d100_d400.robust_paths import PLOTS_DIR\n",
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_education.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980af751",
   "metadata": {},
   "source": [
    "**Partial Dependence: Education**\n",
    "\n",
    "Education appears to be approximately linear in both models. It's a good example of not needing a particularly powerful model like LGBM to capture this relationship, a simple regression model like GLM does a good job. The story that both models tell makes intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659404dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_age.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7aa51",
   "metadata": {},
   "source": [
    "**Partial Dependence: Age**\n",
    "\n",
    "Age is a different story. GLM is struggling to capture the nuance of the data, which LGBM does capture. Intuitively, it makes sense that income peaks at around middle-age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_capital_net.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12f7bd",
   "metadata": {},
   "source": [
    "**Partial Dependence: Net Capital**\n",
    "\n",
    "GLM predicts reduced high-income probability for negative Net Capital, contrasting with LGBM's initially high dependence. LGBM exhibits significant volatility and sharp spikes, suggesting overfitting to sparse data. The true relationship likely lies between these models; intuitively, negative capital implies prior asset ownership, supporting a stronger correlation with high income than the GLM suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce970053",
   "metadata": {},
   "source": [
    "## Future Improvements & Business Considerations\n",
    "\n",
    "### Dataset Improvements\n",
    "\n",
    "Although this dataset was valuable in predicting high income, there could be improvements. I list a few below:\n",
    "\n",
    "- Additional features that would likely make our predictions more accurate:\n",
    "    - Geographic granularity\n",
    "        - State/city of residence (not just country)\n",
    "        - Cost of living index for location\n",
    "        - Local unemployment rate\n",
    "    - Detailed education information\n",
    "        - Specific school/university attended\n",
    "        - Subject/s specialised at university\n",
    "    - Company Information:\n",
    "        - Industry sector employed at\n",
    "        - Company size (small business vs Fortune 500)\n",
    "        - Years at current employer\n",
    "\n",
    "- Target variable\n",
    "    - Binary >USD 50k or <=USD 50k was the only income metric available in the dataset\n",
    "    - Although a high income predictor is valuable, a low income predictor would also have been valuable\n",
    "    - Predicting the exact income, rather than binary could have been more valuable\n",
    "\n",
    "- Cleaner data\n",
    "    - Though I was able to clean the data pretty well, there were some issues that could not be fixed:\n",
    "        - Missing data in occupation and native-country\n",
    "        - Unknown unknowns. It's possible that there were more issues with the data that I failed to find\n",
    "\n",
    "- More data\n",
    "    - Although the sample size was relatively large, it was sparse for some edge cases, like non-zero net capital individuals. As a result, the LGBM model likely overfitted for non-zero net capital individuals\n",
    "    - The data came from a relatively old source, the 1994 Census. Additional census years would provide more confidence in the results, rather than being 1994 specific\n",
    "    - Multiple Censuses might help in spotting time trends, to predict future high incomers\n",
    "\n",
    "- Relevant data\n",
    "    - Most business cases for predicting high income are likely to be interested in predicting high income today, or in the near future\n",
    "        - A more recent Census would be a stronger data source for achieving this\n",
    "    - Organisations based in the UK would prefer a data source that originated from people in the UK, not the US\n",
    "\n",
    "### Modelling Improvements\n",
    "\n",
    "#### Other Models\n",
    "\n",
    "Only two models were used in this analysis, however, there is a vast array of other alternatives. For example:\n",
    "\n",
    "- **Alternative for GLM: Generalized Additive Models (GAMs)**\n",
    "    - *What:* This model can be seen as an extension of the GLM model, where I also allow for non-linear functions\n",
    "    - *Why good for this problem:* Allows for non-linear functions for specific features, this is clearly an improvement on our GLM for features like age; still good degree of interpretability due to additivity\n",
    "    - *Downsides:* Overfitting is more of a risk than GLM; Like GLM, I still have to pre-define potential interactions between features\n",
    "\n",
    "- **Alternative for LGBM: XGBoost**\n",
    "    - *What:* Another gradient boosting model, like LGBM; XGBoost uses depth-wise growth, vs LGBM's tree-wise growth\n",
    "    - *Why good for this problem:* Would naturally handle the non-linear age-income relationship and interactions between education/occupation; robust to the outliers in capital_net; can handles the mix of numeric and categorical features\n",
    "    - *Downsides:* Typically less accurate than LGBM on tabular data like this; slower training and larger model size\n",
    "\n",
    "**Alternative vs GLM/LGBM:**\n",
    "\n",
    "- **GAM likely beats GLM:** For our dataset, with a number of non-linear features, it's likely a GAM model would be preferred over our GLM model. I'd recommend trying GAM on this dataset for future research\n",
    "- **LGBM likely beats XGBoost:** The growth strategy used by LGBM tends to outperform XGBoost's method. Perhaps a more interesting experiment for future research would be a totally different style of model, like a neural network, though this dataset's 48k rows might be too small for such a model\n",
    "\n",
    "#### Advanced Feature Engineering\n",
    "\n",
    "With more time I would have liked to delve into some of the following feature engineering techniques.\n",
    "\n",
    "- **More Manual Feature Interactions (for GLM)**\n",
    "    - Education × Occupation (PhD in healthcare vs PhD in academia)\n",
    "    - Hours_per_week × Occupation (overtime matters differently by job)\n",
    "    - Implementation: Use PolynomialFeatures or manual feature creation\n",
    "\n",
    "- **Polynomial analysis of features**\n",
    "    - Capital_net transformations (log, sqrt) for skewed distribution\n",
    "    - Could use sklearn's PolynomialFeatures() function\n",
    "\n",
    "- **Binning of features**\n",
    "    - Age groups (young adult, mid-career, pre-retirement)\n",
    "    - Hours worked categories (part-time, full-time, overtime)\n",
    "    - This may help capture non-linear effects more explicitly, especially with GLM\n",
    "\n",
    "#### Hyperparameter Tuning Extensions\n",
    "\n",
    "Sklearn offers many other options for tuning GLM and LGBM models that I did not touch on. Had I, perhaps I would have seen a real improvement in the tuned models vs baseline models.\n",
    "\n",
    "### Critical Business Considerations\n",
    "\n",
    "- Since detailed census data is difficult to access, this serves as a 'toy model' for feasibility. In a business context, this stage would help evaluate whether the significant investment required for a production-ready solution is worthwhile.\n",
    "\n",
    "- Success may not require productionalization. The model's outputs, specifically feature importance and partial dependence, can be valuable on their own, such as helping a charity refine its donor wealth assessments.\n",
    "\n",
    "- Ultimately, the goal is business value. If an organization seeks insights rather than a deployed prediction engine, it makes little sense to choose a fancy flexible model that sacrifices interpretability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
