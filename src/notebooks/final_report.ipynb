{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b256319",
   "metadata": {},
   "source": [
    "# Income Prediction from Census Data\n",
    "\n",
    "## Summary\n",
    "This report presents an analysis of the [Census Income dataset](https://archive.ics.uci.edu/dataset/2/adult), predicting whether individuals in the US earn above or below USD 50,000 annually (1994 pricing, adjusting for inflation, this income is equivalent to USD 110,000 in 2025). The analysis encompasses data cleaning, exploratory analysis, feature engineering, model development, and evaluation, culminating in a comparison of Generalized Linear Model (GLM) and Gradient Boosting Machine (GBM) approaches.\n",
    "\n",
    "### Key Results:\n",
    "- Final Models: LGBM achieved the highest accuracy, with 87.4% cross-validated accuracy. However, despite much simpler, GLM still achieved good accuracy, with 84.5%, and may be the best model of the two depending on business case.\n",
    "\n",
    "- Top 5 Predictive Features (LGBM):\n",
    "    - Capital Net: Investment income indicator\n",
    "    - Age: Life-cycle earnings pattern\n",
    "    - Hours per Week: Full-time vs part-time distinction\n",
    "    - Education: Human capital investment\n",
    "    - Relationship = Married: Household income dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47391d14",
   "metadata": {},
   "source": [
    "## Motivation for the Prediction Task\n",
    "\n",
    "Accurately predicting individual income levels has significant practical applications across multiple sectors. For example:\n",
    "\n",
    "**Government & Public Policy:**\n",
    "- **Welfare Eligibility Screening:** In countries lacking centralized income tracking (e.g., UK), predictive models can help identify individuals likely eligible for means-tested benefits (Winter Fuel Payment, Universal Credit) without intrusive income verification.\n",
    "\n",
    "**Non-Profit Sector:**\n",
    "- **Donor Prospecting:** Charities often use crude proxies (e.g., age, postcode) for wealth assessment during door-to-door fundraising. Data-driven income prediction can improve targeting efficiency, reducing wasted outreach while respecting privacy.\n",
    "\n",
    "**Privacy Consideration:** This approach offers an alternative to directly collecting sensitive income data, this may give an edge to sertain businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb81bd",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Dataset Overview\n",
    "The Census Income dataset from the UCI Machine Learning Repository contains demographic and employment information extracted from the 1994 US Census. The prediction task is to determine whether an individual's income exceeds USD 50,000 per year.\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Samples: 48,842 individuals\n",
    "- Original Features: 15 variables\n",
    "- Target Variable: Binary income classification (<=50K, >50K)\n",
    "- Source: US Census Bureau, 1994\n",
    "\n",
    "**Distribution**:\n",
    "\n",
    "Not all numerical columns distribute similarly. While age shows a relatively smooth spread, hours-per-week is highly concentrated with a single value (40 hours) representing 46.7% of the dataset. This concentration makes sense, as 40 hours is the standard full-time employment, whereas age varies naturally across the population. Understanding these distribution patterns is essential for appropriate statistical analysis and modelling decisions. The chart below highlights the diversity in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from income_predict_d100_d400.robust_paths import DATA_DIR\n",
    "from income_predict_d100_d400.final_report import plots\n",
    "\n",
    "parquet_path = DATA_DIR / \"census_income.parquet\"\n",
    "\n",
    "df_raw = pd.read_parquet(parquet_path)\n",
    "\n",
    "plots.distribution_variety(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b3a0f",
   "metadata": {},
   "source": [
    "### Initial Data Quality Assessment\n",
    "\n",
    "There were a number of minor data quality issues with the dataset, many of which could be resolved by simple transformations of the dataset. Below I visualise some of the data quality problems that needed to be dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.visualize_missing_data(df_raw)\n",
    "plots.binary_column_issue(df_raw, 'income', expected_values=['<=50K', '>50K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad753f",
   "metadata": {},
   "source": [
    "#### Cleaning Pipeline\n",
    "\n",
    "Below I list all cleaning steps taken:\n",
    "- Add unique ID column (there were 29 duplicates, this isn't unexpected as some people will have the same demographics. Adding unique ID allows us to tell these individuals apart)\n",
    "- Clean column names (e.g. 'capital-gain' -> 'capital_gain')\n",
    "- Remove seemingly redundant columns ('fnlwgt', 'education-num')\n",
    "- Clean income data (there were some values with a '.' at the end e.g. '>50K.') \n",
    "- Binarize income data (1 for '>50K', 0 for '<=50K') and rename to 'high_income'\n",
    "- Binarize sex and rename to is_female (1 for 'Female, 0 for 'Male')\n",
    "- Binarize race into two columns, is_white and is_black\n",
    "- Replace '?' values with pandas NaN\n",
    "- Trim whitespace from all string values\n",
    "- Order and transform 'education' column. Change datatype from str to int (e.g. 'Preschool' = 1 < '1st-4th' = 2 < ... < 'Doctorate' = 16)\n",
    "- Transform capital-gain and capital-loss into a single column capital_net\n",
    "- Relationship field was slightly simplified, bundling 'Wife' and 'Husband' into one value, 'Married'\n",
    "\n",
    "All cleaning steps were split modularly into their own functions in the file cleaning.py. They can be run together using the full_clean function in cleaning.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from income_predict_d100_d400 import cleaning\n",
    "\n",
    "df_clean = cleaning.full_clean(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce7dff",
   "metadata": {},
   "source": [
    "## Feature Selection & Engineering\n",
    "\n",
    "### Feature Engineering Approach\n",
    "\n",
    "The cleaned dataset includes several derived features: `capital_net` combines capital gains and losses, `is_white`, `is_black`, and `is_female` binarize demographic categories, and `education` is ordinally encoded to reflect progression. For the GLM specifically, an interaction term `age_x_education` was added to capture how the effect of education on income varies with age. We manually create this because, unlike LGBMs that can learn this implicitly, GLMs require explicit features.\n",
    "\n",
    "### Feature Selection\n",
    "The rational was largely based on intuition and prior literature, though some simple exploratory analysis was also used to confirm such theories.\n",
    "\n",
    "#### Numeric Features:\n",
    "- **age** (Strong non-linear association with income (see visualisation below))\n",
    "- **education** (With the ordinal encoding)\n",
    "- **capital_net**\n",
    "- **hours_per_week**\n",
    "- **is_female**\n",
    "- **is_white**\n",
    "- **is_black**\n",
    "- **age_x_education** (help GLM recognise cross-feature correlation)\n",
    "\n",
    "#### Categorical Features:\n",
    "- **work_class**\n",
    "- **occupation**\n",
    "- **relationship**\n",
    "- **native_country**\n",
    "\n",
    "#### Features Excluded and Why:\n",
    "- **fnlwgt**: Sampling weight for census data, not predictive of individual income\n",
    "- **education-num**: Redundant with categorical education (now ordinal education)\n",
    "- **marital_status**: Excluded to avoid multicollinearity with relationship\n",
    "- **capital_gain / capital_loss**: Combined into capital_net to reduce dimensionality\n",
    "- **unique_id**: There should be no correlation between the way the IDs were generated and people's income (see visualisation below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027349ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.correlation_compare(df_clean)\n",
    "plots.occupation_correlation(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523877e9",
   "metadata": {},
   "source": [
    "### Train-Test Split Strategy\n",
    "\n",
    "**Split Methodology**:\n",
    "- Method: Hash-based splitting on unique_id\n",
    "- Ratio: 80% train, 20% test\n",
    "- Reproducibility: Hash function ensures same split across runs\n",
    "\n",
    "**Rationale**:\n",
    "- Hash-based splitting avoids random seed dependence\n",
    "- Same individual always in same set (no data leakage)\n",
    "- 80/20 provides sufficient training data while maintaining robust test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3dc7c",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Two complementary approaches were selected:\n",
    "\n",
    "1. Generalized Linear Model (GLM) - Logistic Regression\n",
    "2. LightGBM (Gradient Boosting Machine)\n",
    "\n",
    "**Why GLM?**:\n",
    "\n",
    "- Interpretability: Coefficients show linear relationship between each feature and the target\n",
    "- Baseline: Establishes performance floor. Simpler models preferred if comparable\n",
    "- Probabilistic: Natural probability outputs for binary classification\n",
    "- Fast Training: Scales well to large datasets\n",
    "- Assumptions: Appropriate if relationships are approximately linear\n",
    "\n",
    "**Why LGBM?**:\n",
    "\n",
    "- Non-linear Relationships: Captures complex interactions between features\n",
    "- Feature Interactions: Automatically learns combinations without manual engineering\n",
    "- Robust: Handles missing values, outliers, and mixed feature types well\n",
    "- Performance: Generally highly accurate, in comparisson to GLM and other boosting models\n",
    "- Feature Importance: Provides interpretable feature rankings\n",
    "\n",
    "### Evaluation Approach\n",
    "\n",
    "#### Metric Selection\n",
    "\n",
    "Metric Selection and Rationale:\n",
    "\n",
    "| Metric | Why Important | Interpretation |\n",
    "|--------|---------------|----------------|\n",
    "| **Accuracy** | Overall correctness | % of correct predictions |\n",
    "| **Precision** | Minimize false positives | Of predicted high-income, % actually high-income |\n",
    "| **Recall** | Minimize false negatives | Of actual high-income, % correctly identified |\n",
    "| **F1-Score** | Balance precision/recall | Harmonic mean; useful for imbalanced data |\n",
    "| **ROC-AUC** | Ranking quality | Probability of ranking random positive > negative |\n",
    "| **Log Loss** | Probability calibration | Penalizes confident wrong predictions |\n",
    "\n",
    "\n",
    "**Primary Metric/s**: It depends on the exact business question. For example, charity donor prospecting will likley want a model with high **Precision** and **Log Loss**, so that they can target people that are most likley to have high income. Another example would be for government public policy that is keen to understand where both high income and not high income people are, **Accuracy** would be the most valuable metric to them.\n",
    "\n",
    "#### Evaluation Strategy\n",
    "\n",
    "- Train Set: Used for model training and hyperparameter tuning (with cross-validation)\n",
    "- Test Set: Held out for final evaluation only (no model selection on test set)\n",
    "- Cross-Validation: 5-fold stratified CV during hyperparameter tuning\n",
    "    - Ensures robust parameter selection\n",
    "    - Reduces overfitting to single validation split\n",
    "    - Maintains class balance in each fold\n",
    "- Preventing Overfitting:\n",
    "    - Separate train/test split (no test set leakage)\n",
    "    - Cross-validation for hyperparameter selection\n",
    "    - Regularization in GLM (L1/L2 penalty)\n",
    "    - Early stopping consideration in LGBM\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "#### GLM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `alpha` | 10^-4 to 10^-1 (log-uniform) | Regularization strength (higher = more regularization) | 0.0001 | **0.000861** |\n",
    "| `l1_ratio` | 0 to 1 (uniform) | Balance between L1 (Lasso) and L2 (Ridge) penalty | 0.15 | **0.520** |\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (more efficient than grid search for continuous parameters)\n",
    "- **Iterations:** 50 random combinations sampled\n",
    "- **Cross-Validation:** 5-fold stratified ensures robust evaluation\n",
    "- **Search Strategy:** Log-uniform for `alpha` (covers multiple orders of magnitude)\n",
    "\n",
    "#### LGBM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `learning_rate` | 0.01 to 0.2 (log-uniform) | Controls step size in gradient descent | 0.1 | **0.158** |\n",
    "| `n_estimators` | 50 to 200 (integer) | Number of boosting rounds (trees) | 100 | **89** |\n",
    "| `num_leaves` | 10 to 60 (integer) | Maximum leaves per tree (complexity) | 31 | **30** |\n",
    "| `min_child_weight` | 0.0001 to 0.002 (log-uniform) | Minimum sum of instance weight in leaf (regularization) | 0.001 | **0.00013** |\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (efficient for large search space)\n",
    "- **Iterations:** 20 combinations (LGBM slower to train than GLM)\n",
    "- **Cross-Validation:** 5-fold stratified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f095fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077329cc",
   "metadata": {},
   "source": [
    "**Performance Interpretation:**\n",
    "\n",
    "**LGBM outperforms GLM across nearly all metrics.** The tuned LGBM achieves lower error rates (MSE: 0.087 vs 0.107, MAE: 0.174 vs 0.217) and better discriminative ability (Gini: 0.654 vs 0.602). Accuracy follows the same pattern at 87.3% vs 84.6%.\n",
    "\n",
    "**The most striking result is GLM's bias reduction after tuning.** Bias dropped from 0.053 to 0.011 (80% improvement). This means the baseline GLM was systematically over-predicting (mean prediction 0.246 vs actual 0.234), and tuning corrected this calibration issue. However, other GLM metrics barely changed, suggesting the tuning primarily improved calibration rather than overall predictive power.\n",
    "\n",
    "**LGBM showed modest gains from tuning**, with small improvements across metrics. This indicates the baseline LGBM hyperparameters were already near-optimal for this dataset, while GLM benefited more substantially from parameter adjustment.\n",
    "\n",
    "Given the general improvements in the tuned models, I shall only focus on these models moving forward.\n",
    "\n",
    "**Classification Performance Analysis**\n",
    "\n",
    "Classification performance matters because it reveals not just overall accuracy, but where the model is accurate, i.e. false positives vs false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabedee",
   "metadata": {},
   "source": [
    "**Confusion Matrix Insights**:\n",
    "\n",
    "LGBM beats GLM across all metrics in the classification analysis.\n",
    "\n",
    "Given LGBM is better at avoiding false positives and false negatives, there is no need to consider which is more important. However, if one model was better at false positives and the other better at false negatives, we would need to consider the business implications to determine which model would be prefered.\n",
    "\n",
    "**Feature Importance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d67cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.feature_importance_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663924a5",
   "metadata": {},
   "source": [
    "**Feature Importance Insights**\n",
    "\n",
    "Interestingly, the Feature Importance chart shows that the GLM model and LGBM model can rate particualr features very similarly, in the case of capital_net it's no.2 for GLM and no.1 for LGBM, as well as very differently, in the case of relationship = 'Own-child' it's no.1 for GLM and no.33 for LGBM. This highlights the fundamental differences in how these models learn patterns from data. GLM relies on linear relationships and assigns high importance to features with strong individual correlations to the target, whereas LGBM captures complex non-linear interactions and may find a feature redundant if its predictive power is already captured by other variables. The discrepancy with 'Own-child' suggests that while this relationship status has a clear association with income, LGBM likely extracts the same signal through combinations of other features like age and hours worked.\n",
    "\n",
    "**Partial Dependence Analysis**\n",
    "\n",
    "The four Partial Dependence charts below may seem exissive, however, I do think they are all valuable in understanding both how GLM and LGBM models work generally, and the specific insights they highlight for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "from income_predict_d100_d400.robust_paths import PLOTS_DIR\n",
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_education.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980af751",
   "metadata": {},
   "source": [
    "**Partial Dependence: Education**\n",
    "\n",
    "Education appears to be approximatley linear in both models. It's a good example of not needing a particularly powerful model like LGBM to capture this relationship, a simple regression model like GLM does the job. Furthermore, the story that both models say makes intuitive sense, since it is generally understood that the more education you have the more likley you are to earn a higher paying job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659404dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_age.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7aa51",
   "metadata": {},
   "source": [
    "**Partial Dependence: Age**\n",
    "\n",
    "Age is a different story. GLM is clearly faily to capture the nuance of the data, which LGBM does capture. Intuitivley, it makes sense that income peaks at around middle-age.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_capital_net.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12f7bd",
   "metadata": {},
   "source": [
    "**Partial Dependence: Net Capital**\n",
    "\n",
    "Net Capital offers us another interesting story. While GLM predicts a straightforward linear relationship between net capital and high income, an intuitively reasonable assumption, LGBM exhibits peculiar behavior. It assigns very low probability of high income at USD 3,500 net capital but very high probability at USD 3,000.\n",
    "\n",
    "This anomaly likely comes from limitations in the data. The vast majority of observations have zero net capital, leaving sparse data for model training at other values. An alternative reason could be a genuine quirk in the 1994 US dataset. Perhaps a policy or event that year resulted in many high earners receiving exactly USD 3,000 in net capital.\n",
    "\n",
    "Regardless of cause, this example highlights LGBM's remarkable ability to detect patterns at specific values. While this can lead to false patterns when data is limited, you can imagine it being extremely useful for tasks like fraud detection, where fraudsters might consistently use certain, seemingly-random figures that would be hard for humans to notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97733024",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=PLOTS_DIR / \"partial_dependence_relationship.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1ae59",
   "metadata": {},
   "source": [
    "**Partial Dependence: Relationship**\n",
    "\n",
    "Our final story in the Partial Dependence analysis comes from the Relationship field. Unlike the other features show above, this is a categorical feature. LGBM and GLM give approximatley similar values for each of the Relationship categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce970053",
   "metadata": {},
   "source": [
    "## Future Improvements & Business Considerations\n",
    "\n",
    "### Dataset Improvements\n",
    "\n",
    "Although this dataset was valuable in predicting high income, there could be improvements. I list a few below:\n",
    "\n",
    "- Additional features that would likley make our predictions more accurate:\n",
    "    - Geographic granularity\n",
    "        - State/city of residence (not just country)\n",
    "        - Cost of living index for location\n",
    "        - Local unemployment rate\n",
    "    - Detailed education information\n",
    "        - Specific school/university attended\n",
    "        - Subject/s speciallised at university\n",
    "    - Company Information:\n",
    "        - Industry sector employed at\n",
    "        - Company size (small business vs Fortune 500)\n",
    "        - Years at current employer\n",
    "\n",
    "- Target variable\n",
    "    - Binary >USD 50k or <=USD 50k was the only income metric avaialble in the dataset\n",
    "    - Although a USD 50,000 income at the time of the Census was quite a high income, I might have adjusted this thershold slightly has there been data available to do so\n",
    "    - It may have been more business relevant to predict the exact income of an individual, rather than the binary prediction. To do this I would need the dataset to be enhanced with exact incomes of individuals\n",
    "\n",
    "- Cleaner data\n",
    "    - Though I was able to clean the data pretty well, there were some issues that could not be fixed:\n",
    "        - Missing data in occuption and native-country\n",
    "        - Unknown unknowns. It's possible that there were more issues with the data that I failed to find\n",
    "\n",
    "- More data\n",
    "    - Although the sample size was relativley large, it was sparse for some edge cases, like non-zero net capital individuals. As a result, the LGBM model likley overfitted for non-zero net capital individuals\n",
    "    - The data came from a relativley old source, 1994 Census. Additonal data from other census years would provide more confidence in the results being general, rather than 1994 specific\n",
    "\n",
    "- Relevant data\n",
    "    - Most business cases for predicting high income are likley to be interested in predicting high income today, or in the near future\n",
    "        - The most recent Census would be a much stronger data source for achieving this\n",
    "        - Multiple Censuses might help in spotting time trends, to predict future high incomers\n",
    "    - Organisations based in the UK would prefer a data source that originated from people in the UK, not the US\n",
    "\n",
    "### Modelling Improvements\n",
    "\n",
    "#### Other Models\n",
    "\n",
    "Only two models were used in this analysis, however, there is a vast array of other alternatives. For example:\n",
    "\n",
    "- **Alternative for GLM: Generalized Additive Models (GAMs)**\n",
    "    - *What:* This model can be seen as an extension of the GLM model, where we also allow for non-linear functions\n",
    "    - *Why good for this problem:* Allows for non-linear functions for specific features, this is clearly an improvement on our GLM for features like age; still good degree of interpretability due to additivity\n",
    "    - *Downsides:* Overfitting is more of a risk than GLM; Like GLM, we still have to pre-define potential interactions between features\n",
    "\n",
    "- **Alternative for LGBM: XGBoost**\n",
    "    - *What:* Another gradient boosint model, like LGBM; XGBoost uses depth-wise growth, vs LGBM's tree-wise growth\n",
    "    - *Why good for this problem:* Would naturally handle the non-linear age-income relationship and interactions between education/occupation; robust to the outliers in capital_net; can handles the mix of numeric and categorical features\n",
    "    - *Downsides:* Typically less accurate than LGBM on tabular data like this; slower training and larger model size\n",
    "\n",
    "**Alternative vs GLM/LGBM:**\n",
    "\n",
    "- **GAM likley beats GLM:** For our dataset, with a number of non-linear features, it's likely a GAM model would be preffered over our GLM model. I'd recommend trying GAM on this dataset for furture research\n",
    "- **LGBM likley beats XGBoost:** The growth strategy used by LGBM tends to outperform XGBoost's method. Perhaps a more interesting experiment for future research would be a totally differnt style of model, like a neural network, though this dataset's 48k rows might be too small for such a model\n",
    "\n",
    "#### Advanced Feature Engineering\n",
    "\n",
    "With more time I would have liked to delve into some of the following feature engineering techniques.\n",
    "\n",
    "- **More Manual Feature Interactions (for GLM)**\n",
    "    - Education × Occupation (PhD in healthcare vs PhD in academia)\n",
    "    - Hours_per_week × Occupation (overtime matters differently by job)\n",
    "    - Implementation: Use PolynomialFeatures or manual feature creation\n",
    "\n",
    "- **Polynomial anlysis of features**\n",
    "    - Capital_net transformations (log, sqrt) for skewed distribution\n",
    "    - Could use sklearn's PolynomialFeatures() function\n",
    "\n",
    "- **Binning of features**\n",
    "    - Age groups (young adult, mid-career, pre-retirement)\n",
    "    - Hours worked categories (part-time, full-time, overtime)\n",
    "    - This may help capture non-linear effects more explicitly, especially with GLM\n",
    "\n",
    "#### Hyperparameter Tuning Extensions\n",
    "\n",
    "Sklearn offers many other options for tuning GLM and LGBM models that I did not touch on. Had I, perhaps I would have seen a real improvement in the tuned models vs baseline models.\n",
    "\n",
    "### Critical Business Considerations\n",
    "\n",
    "As mentioned previously, access to more relevant census data would be much more valuable for most organisations interested in this sort of analysis.\n",
    "\n",
    "Moving beyond this limitation, one has to wonder how organisations can deploy such a model. Detailed enough census data is difficult to access (hence why I use 1994 US census). In the real business world I'd expect this model to be a sort of toy model, used as part of the evaluation of whether significant investment towards a model that can be productionalised is worth it.\n",
    "\n",
    "On the other hand, perhaps productionalization of a model isn't required for this model/anlaysis to be deemed a success. It is possible that the insights generated, e.g. feature importance and partial dependence, is where the value lays. For example, a charity may update its potential donor wealth assessment because if this.\n",
    "\n",
    "It's important to keep in mind that the whole reason for data science in business is to generate business value. If an organisation is interested in the insights of a data science project, rather than deploying a model into production, it makes little sense to select a model with high flexibility but low interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
