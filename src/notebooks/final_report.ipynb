{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b256319",
   "metadata": {},
   "source": [
    "# Income Prediction from Census Data\n",
    "\n",
    "## Summary\n",
    "This report presents a comprehensive analysis of the [Census Income dataset](https://archive.ics.uci.edu/dataset/2/adult), predicting whether individuals earn above or below $50,000 annually (1994 pricing). The analysis encompasses data cleaning, exploratory analysis, feature engineering, model development, and evaluation, culminating in a comparison of Generalized Linear Model (GLM) and Gradient Boosting Machine (GBM) approaches.\n",
    "\n",
    "### Key Results:\n",
    "- Final Model: LightGBM achieved 87.4% cross-validated accuracy on test set with excellent probability calibration (Gini coefficient 0.654, AUC ≈ 0.827)\n",
    "\n",
    "- Top Predictive Features:\n",
    "    - Age (554 splits) - Life-cycle earnings pattern\n",
    "    - Capital Net (527 splits) - Investment income indicator\n",
    "    - Hours per Week (309 splits) - Full-time vs part-time distinction\n",
    "    - Education (268 splits) - Human capital investment\n",
    "    - Relationship Married (66 splits) - Household income dynamics\n",
    "\n",
    "- Model Performance:\n",
    "    - 18-19% error reduction vs GLM baseline (MAE: 0.174 vs 0.216)\n",
    "    - Excellent calibration: Only 1.6% bias (predictions match actual rates)\n",
    "    - Strong ranking ability: Gini 0.654 enables effective targeting of top prospects\n",
    "    - Outperforms GLM across all 8 evaluation metrics with substantial margins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47391d14",
   "metadata": {},
   "source": [
    "\n",
    "## Motivation for the Prediction Task\n",
    "\n",
    "Accuratley predicting individual income levels has significant practical applications across multiple sectors:\n",
    "\n",
    "**Government & Public Policy:**\n",
    "- **Welfare Eligibility Screening:** In countries lacking centralized income tracking (e.g., UK), predictive models can help identify individuals likely eligible for means-tested benefits (Winter Fuel Payment, Universal Credit) without intrusive income verification.\n",
    "- **Targeted Outreach:** Enables efficient allocation of resources by identifying populations most likely to need assistance programs.\n",
    "\n",
    "**Non-Profit Sector:**\n",
    "- **Donor Prospecting:** Charities often use crude proxies (e.g., age, postcode) for wealth assessment during door-to-door fundraising. Data-driven income prediction can improve targeting efficiency, reducing wasted outreach while respecting privacy.\n",
    "- **Campaign Optimization:** Identify high-value donor prospects for major gift campaigns vs. mass-market appeals.\n",
    "\n",
    "**Financial Services:**\n",
    "- **Customer Acquisition:** Banks, accountants, and financial advisers see higher demand from affluent clients. Income prediction enables cost-effective targeting of premium services through digital advertising platforms (Facebook, LinkedIn) that offer rich demographic targeting.\n",
    "- **Product Recommendation:** Tailor financial product offerings (premium credit cards, investment services) based on predicted income segment.\n",
    "- **Credit Risk Assessment:** Income prediction serves as an input to credit scoring models when direct income verification is unavailable.\n",
    "\n",
    "**Privacy Consideration:** This approach offers an alternative to directly collecting sensitive income data, using publicly available demographic features to make probabilistic inferences—balancing business needs with individual privacy.\n",
    "\n",
    "**This Analysis:** Using 1994 US Census data, we develop and evaluate income prediction models to demonstrate the feasibility and accuracy of this approach, with insights transferable to modern applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb81bd",
   "metadata": {},
   "source": [
    "## Explanatory data analysis\n",
    "\n",
    "### Dataset Overview\n",
    "The Census Income dataset from the UCI Machine Learning Repository contains demographic and employment information extracted from the 1994 US Census. The prediction task is to determine whether an individual's income exceeds $50,000 per year; adjusting for inflation, this income is equivelent to $110,000 in 2025.\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Samples: 48,842 individuals\n",
    "- Original Features: 15 variables\n",
    "- Target Variable: Binary income classification (<=50K, >50K)\n",
    "- Source: US Census Bureau, 1994\n",
    "\n",
    "**Coulmn Description**\n",
    "\n",
    "| Column | Type | Description | Values/Range | Cleaned Version |\n",
    "|---------|------|-------------|--------------|-----------------|\n",
    "| age | Numeric | Age in years | 17-90 | No change |\n",
    "| workclass | Categorical | Employment sector | Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked | rename to work_class |\n",
    "| fnlwgt | Numeric | Final sampling weight | - | **Dropped** |\n",
    "| education | Categorical | Highest education level | Preschool, 1st-4th, 5th-6th, 7th-8th, 9th, 10th, 11th, 12th, HS-grad, Some-college, Assoc-voc, Assoc-acdm, Bachelors, Masters, Prof-school, Doctorate | Manually ordered values and converted to integer. 1 = lowest level of education, 16 = highest level |\n",
    "| education-num | Numeric | Education in years | - | **Dropped** |\n",
    "| marital-status | Categorical | Marital status | - | **Dropped** |\n",
    "| occupation | Categorical | Job type | Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces | rename to occupation |\n",
    "| relationship | Categorical | Family relationship | Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried | rename to  relationship, transformed Husband/Wife values to Married |\n",
    "| race | Categorical | Race | White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black | Transformed to two binary columns, is_white and is_black |\n",
    "| sex | Categorical | Sex | Male, Female | Transformed to a binary column, is_female |\n",
    "| capital-gain | Numeric | Capital gains | 0-99999 | **Combined** into capital_net |\n",
    "| capital-loss | Numeric | Capital losses | 0-4356 | **Combined** into capital_net |\n",
    "| hours-per-week | Numeric | Hours worked per week | 1-99 | rename to hours_per_week |\n",
    "| native-country | Categorical | Country of birth | United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands | rename to  native_country |\n",
    "| income | Categorical | Annual income | <=50K, >50K | Fixed bad labels. Converted to boolean and rename to **high_income** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "src_directory = current_dir.parent\n",
    "sys.path.append(str(src_directory))\n",
    "\n",
    "from notebooks.visualisations import display_dataset, distribution_variety\n",
    "\n",
    "parquet_path = src_directory / \"data\" / \"census_income.parquet\"\n",
    "\n",
    "df_raw = pd.read_parquet(parquet_path)\n",
    "\n",
    "distribution_variety.distribution_variety(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390c41b",
   "metadata": {},
   "source": [
    "**Distribution Insights**:\n",
    "\n",
    "Not all numerical columns distribute similarly. While age shows a relatively smooth spread, hours-per-week is highly concentrated with a single value (40 hours) representing 46.7% of the dataset. This concentration makes sense, as 40 hours is the standard full-time employment, whereas age varies naturally across the population. Understanding these distribution patterns is essential for appropriate statistical analysis and modeling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b3a0f",
   "metadata": {},
   "source": [
    "### Initial Data Quality Assessment\n",
    "\n",
    "There were a number of minor data quality issues with the dataset, many of which could be resolved by simple transformations of the dataset. Below visualise some of the data quality problems that needed to be dealt with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.visualisations import visualize_missing_data, binary_column_issue\n",
    "\n",
    "visualize_missing_data.visualize_missing_data(df_raw)\n",
    "binary_column_issue.binary_column_issue(df_raw, 'income', expected_values=['<=50K', '>50K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad753f",
   "metadata": {},
   "source": [
    "Below list all cleaning steps taken:\n",
    "- Add unique ID column (there were 29 duplicates, this isn't unexpected as some people will have the same demographics. Adding unique ID allows us to tell these individuals appart)\n",
    "- Clean column names (e.g. 'capital-gain' -> 'capital_gain')\n",
    "- Remove seemingly redundant columns ('fnlwgt', 'education-num')\n",
    "- clean income data (there were some values with a '.' at the beginning e.g. '>50K.') \n",
    "- binarize income data (1 for '>50K', 0 for '=<50K') and rename to 'high_income'\n",
    "- Replace '?' values with pandas NaN\n",
    "- trim whitespace from all str values\n",
    "- Order and transform 'education' column. Change datatype from str into int (e.g. 'Preschool' = 1 < '1st-4th' = 2 < ... < 'Doctorate' = 16)\n",
    "- Transform capital-gain and capital-loss into a single column capital-net\n",
    "- Relationship field was slightly simplified, bundling 'Wife' and 'Husband' into one value, 'Married'\n",
    "\n",
    "All cleaning steps were split modularly into their own functions in the file cleaning.py. Can be run together using the full_clean function in cleaning.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from income_predict import cleaning\n",
    "\n",
    "df_clean = cleaning.full_clean(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce7dff",
   "metadata": {},
   "source": [
    "## Feature Selection & Engineering Selection\n",
    "\n",
    "### Feature Selection Rationale\n",
    "\n",
    "#### Numeric Features - Why Included:\n",
    "- **age**: Strong correlation with income (see visualisation below)\n",
    "- **education**: Ordinal encoding preserves natural ordering; highest correlation with income\n",
    "- **capital_net**: Combined feature captures investment income; strong predictor for high earners\n",
    "- **hours_per_week**: Work hours correlate with income; distinguishes full-time vs part-time\n",
    "\n",
    "#### Categorical Features - Why Included:\n",
    "- **work_class**: Employment sector (private/government/self-employed) affects income\n",
    "- **occupation**: Job type strongly associated with income levels (see visualisation below)\n",
    "- **relationship**: Household role correlated with income (married couples often dual income)\n",
    "- **race**: Demographic factor that may reflect systemic patterns\n",
    "- **sex**: Gender wage gap documented in literature\n",
    "- **native_country**: Immigration status and country of origin may affect opportunities\n",
    "\n",
    "#### Features Excluded and Why:\n",
    "- **fnlwgt**: Sampling weight for census data, not predictive of individual income\n",
    "- **education-num**: Redundant with categorical education (now ordinal education)\n",
    "- **marital_status**: Excluded to avoid multicollinearity with relationship\n",
    "- **capital_gain / capital_loss**: Combined into capital_net to reduce dimensionality\n",
    "- **unique_id**: There should be no correlation with the way the IDs were generated and people's income (see visualisation below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027349ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.visualisations import correlation_compare, occupation_correlation\n",
    "\n",
    "correlation_compare.correlation_compare(df_clean)\n",
    "occupation_correlation.occupation_correlation(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523877e9",
   "metadata": {},
   "source": [
    "### Feature Engineering Approach\n",
    "\n",
    "#### 1. Education Ordinal Encoding\n",
    "```\n",
    "EDUCATION_ORDER = {\n",
    "    \"Preschool\": 1, \"1st-4th\": 2, \"5th-6th\": 3, \"7th-8th\": 4,\n",
    "    \"9th\": 5, \"10th\": 6, \"11th\": 7, \"12th\": 8,\n",
    "    \"HS-grad\": 9, \"Some-college\": 10,\n",
    "    \"Assoc-voc\": 11, \"Assoc-acdm\": 12,\n",
    "    \"Bachelors\": 13, \"Masters\": 14,\n",
    "    \"Prof-school\": 15, \"Doctorate\": 16\n",
    "}\n",
    "```\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "- Education has natural ordering (more education generally leads to higher income)\n",
    "- Ordinal encoding (1-16) preserves this relationship\n",
    "- Allows models to capture monotonic trends more easily than one-hot encoding\n",
    "- Reduces dimensionality compared to 16 binary features\n",
    "\n",
    "#### 2. Capital Net Feature\n",
    "```\n",
    "df['capital_net'] = df['capital_gain'] - df['capital_loss']\n",
    "```\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "- Most people have zero capital gains AND losses (sparse features)\n",
    "- Net capital is what matters for income classification\n",
    "- Reduces two features to one without information loss\n",
    "- Simplifies model by removing highly correlated features\n",
    "\n",
    "#### 3. Relationship Simplification\n",
    "```\n",
    "df['relationship'] = df['relationship'].replace({'Husband': 'Married', 'Wife': 'Married'})\n",
    "```\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "- 'Husband' and 'Wife' represent same relationship type (married)\n",
    "- Gender already captured in separate sex feature\n",
    "- Reduces categories from 6 to 5, simplifying model\n",
    "- Maintains meaningful distinction (married vs single vs other family roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d71905",
   "metadata": {},
   "source": [
    "### Train-Test Split Strategy\n",
    "\n",
    "**Split Methodology**:\n",
    "- Method: Hash-based splitting on unique_id\n",
    "- Ratio: 80% train, 20% test\n",
    "- Reproducibility: Hash function ensures same split across runs\n",
    "\n",
    "**Rationale**:\n",
    "- Hash-based splitting avoids random seed dependence\n",
    "- Same individual always in same set (no data leakage)\n",
    "- 80/20 provides sufficient training data while maintaining robust test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3dc7c",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Two complementary approaches were selected:\n",
    "\n",
    "1. Generalized Linear Model (GLM) - Logistic Regression\n",
    "2. LightGBM (Gradient Boosting Machine)\n",
    "\n",
    "**Why GLM?**:\n",
    "\n",
    "- Interpretability: Coefficients show linear relationship between each feature and the target\n",
    "- Baseline: Establishes performance floor. Simpler models preferred if comparable\n",
    "- Probabilistic: Natural probability outputs for binary classification\n",
    "- Fast Training: Scales well to large datasets\n",
    "- Assumptions: Appropriate if relationships are approximately linear\n",
    "\n",
    "**Why LGBM?**:\n",
    "\n",
    "- Non-linear Relationships: Captures complex interactions between features\n",
    "- Feature Interactions: Automatically learns combinations without manual engineering\n",
    "- Robust: Handles missing values, outliers, and mixed feature types well\n",
    "- Performance: Generally highly accurate, in comparisson to GLM and other boosting models\n",
    "- Feature Importance: Provides interpretable feature rankings\n",
    "\n",
    "### Evaluation Approach\n",
    "\n",
    "#### Metric Selection\n",
    "\n",
    "Metric Selection and Rationale:\n",
    "\n",
    "| Metric | Why Important | Interpretation |\n",
    "|--------|---------------|----------------|\n",
    "| **Accuracy** | Overall correctness | % of correct predictions |\n",
    "| **Precision** | Minimize false positives | Of predicted high-income, % actually high-income |\n",
    "| **Recall** | Minimize false negatives | Of actual high-income, % correctly identified |\n",
    "| **F1-Score** | Balance precision/recall | Harmonic mean; useful for imbalanced data |\n",
    "| **ROC-AUC** | Ranking quality | Probability of ranking random positive > negative |\n",
    "| **Log Loss** | Probability calibration | Penalizes confident wrong predictions |\n",
    "\n",
    "\n",
    "**Primary Metric/s**: It depends on the exact business question. For example, charity donor prospecting will likley want a model with high **Precision** and **Log Loss**, so that they can target people that are most likley to have high income. Another example would be for government public policy that is keen to understand where both high income and not high income people are, **Accuracy** would be the most valuable metric to them.\n",
    "\n",
    "#### Evaluation Strategy\n",
    "\n",
    "- Train Set: Used for model training and hyperparameter tuning (with cross-validation)\n",
    "- Test Set: Held out for final evaluation only (no model selection on test set)\n",
    "- Cross-Validation: 5-fold stratified CV during hyperparameter tuning\n",
    "    - Ensures robust parameter selection\n",
    "    - Reduces overfitting to single validation split\n",
    "    - Maintains class balance in each fold\n",
    "- Preventing Overfitting:\n",
    "    - Separate train/test split (no test set leakage)\n",
    "    - Cross-validation for hyperparameter selection\n",
    "    - Regularization in GLM (L1/L2 penalty)\n",
    "    - Early stopping consideration in LGBM\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "#### GLM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `alpha` | 10^-4 to 10^-1 (log-uniform) | Regularization strength (higher = more regularization) | 0.0001 | **0.000598** |\n",
    "| `l1_ratio` | 0 to 1 (uniform) | Balance between L1 (Lasso) and L2 (Ridge) penalty | 0.15 | **0.663** |\n",
    "\n",
    "**Performance Improvement:**\n",
    "- **Baseline Accuracy:** 84.0%\n",
    "- **Tuned Accuracy:** 84.6%\n",
    "- **Improvement:** +0.6 percentage points\n",
    "\n",
    "**Selected Parameters Interpretation:**\n",
    "- **Alpha = 0.000598:** 6x increase from baseline\n",
    "  - Still relatively low, suggesting model not significantly overfitting\n",
    "  - Slight increase provides better generalization without excessive constraint\n",
    "  \n",
    "- **L1 Ratio = 0.663:** Elastic Net with stronger L1 (Lasso) component\n",
    "  - Strong shift from baseline (0.15 to 0.663) indicates benefit of L1 regularization\n",
    "  - Promotes sparsity: some features get zeroed coefficients (automatic feature selection)\n",
    "  - L1 component (66.3%) encourages simpler model by eliminating weak predictors\n",
    "  - L2 component (33.7%) still helps with correlated features (e.g., one-hot encoded categories)\n",
    "\n",
    "**Why This Configuration Works:**\n",
    "- Higher L1 ratio eliminates less important one-hot encoded categories\n",
    "- Moderate alpha prevents overfitting while retaining strong predictors\n",
    "- Elastic Net (mix of L1/L2) balances feature selection with stability\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (more efficient than grid search for continuous parameters)\n",
    "- **Iterations:** 50 random combinations sampled\n",
    "- **Cross-Validation:** 5-fold stratified ensures robust evaluation\n",
    "- **Search Strategy:** Log-uniform for `alpha` (covers multiple orders of magnitude)\n",
    "\n",
    "#### LGBM Tuning\n",
    "\n",
    "**Parameters selected**:\n",
    "| Parameter | Search Space | Purpose | Baseline Value | Tuned Value |\n",
    "|-----------|--------------|---------|----------------|----------------|\n",
    "| `learning_rate` | 0.01 to 0.2 (log-uniform) | Controls step size in gradient descent | 0.1 | **0.158** |\n",
    "| `n_estimators` | 50 to 200 (integer) | Number of boosting rounds (trees) | 100 | **89** |\n",
    "| `num_leaves` | 10 to 60 (integer) | Maximum leaves per tree (complexity) | 31 | **30** |\n",
    "| `min_child_weight` | 0.0001 to 0.002 (log-uniform) | Minimum sum of instance weight in leaf (regularization) | 0.001 | **0.00013** |\n",
    "\n",
    "**Performance Comparison:**\n",
    "- **Baseline Accuracy:** 87.5%\n",
    "- **Tuned Accuracy:** 87.4%\n",
    "- **Change:** -0.1 percentage points (negligible decrease)\n",
    "\n",
    "**Selected Parameters Interpretation:**\n",
    "- **Learning Rate = 0.158:** Higher than baseline (0.1 to 0.158)\n",
    "  - 58% increase in step size suggests faster convergence preferred\n",
    "  - Higher learning rate often pairs with fewer trees to prevent overfitting\n",
    "  - Allows model to learn more aggressively per iteration\n",
    "  \n",
    "- **N_estimators = 89:** Fewer trees than baseline (100 → 89)\n",
    "  - Reduction of 11 trees compensates for higher learning rate\n",
    "  - Trade-off: higher LR × fewer trees ≈ similar total learning\n",
    "  - Slightly more efficient (fewer trees = faster inference)\n",
    "  \n",
    "- **Num_leaves = 30:** Slightly simpler trees (31 to 30)\n",
    "  - Minimal change indicates baseline complexity was appropriate\n",
    "  - Less complex trees reduce overfitting risk\n",
    "  - Combined with higher LR, suggests preference for many simple trees\n",
    "  \n",
    "- **Min_child_weight = 0.00013:** Much lower regularization (0.001 to 0.00013)\n",
    "  - ~87% reduction in minimum weight constraint\n",
    "  - Allows model to fit smaller, more specific patterns\n",
    "  - Higher learning rate may provide sufficient regularization via early stopping effect\n",
    "\n",
    "**Why Tuning Didn't Improve Performance:**\n",
    "- Baseline parameters were already well-calibrated for this dataset\n",
    "- LGBM defaults are generally robust across many problems\n",
    "- Slight decrease (-0.08%) within noise, essentially same performance\n",
    "\n",
    "**Tuning Process:**\n",
    "- **Method:** Randomized search (efficient for large search space)\n",
    "- **Iterations:** 20 combinations (LGBM slower to train than GLM)\n",
    "- **Cross-Validation:** 5-fold stratified\n",
    "\n",
    "#### Model Comparison: Tuned GLM vs Tuned LGBM\n",
    "\n",
    "**Performance Summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f095fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.visualisations import model_comparrison\n",
    "\n",
    "model_comparrison.model_comparrison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077329cc",
   "metadata": {},
   "source": [
    "**Performance Interpretation:**\n",
    "\n",
    "**Winner: LGBM dominates across all metrics**\n",
    "\n",
    "- **Bias (5.53% vs 1.61%):** GLM overestimates high-income probability by 5.5%, while LGBM only by 1.6%\n",
    "  - LGBM predictions much better calibrated to actual prevalence\n",
    "  - GLM's linear assumptions may cause systematic bias\n",
    "  \n",
    "- **MSE/RMSE:** LGBM reduces squared error by 18.6%\n",
    "  - LGBM penalizes large errors less (better worst-case performance)\n",
    "  - Suggests LGBM handles outliers and edge cases better\n",
    "  \n",
    "- **MAE (0.216 vs 0.174):** LGBM reduces average absolute error by 19.4%\n",
    "  - On average, LGBM predictions are 4.2 percentage points closer to truth\n",
    "  - Substantial practical improvement in prediction accuracy\n",
    "\n",
    "- **Gini (0.602 vs 0.654):** LGBM's 8.6% improvement in discrimination\n",
    "  - Gini = 2×AUC - 1, so LGBM has higher AUC (0.827 vs 0.801)\n",
    "  - Better at ranking individuals by income probability\n",
    "  - Important for applications like targeted marketing\n",
    "\n",
    "- **Deviance/Log Loss (0.335 vs 0.274):** LGBM's 18.2% improvement\n",
    "  - Lower log loss = better probability estimates\n",
    "  - Critical if using probabilities for decision-making (not just classification)\n",
    "  - LGBM more confident in correct predictions, less confident in wrong ones\n",
    "\n",
    "**Why LGBM Outperforms GLM:**\n",
    "1. **Non-linear relationships:** Can capture age, education interactions, threshold effects\n",
    "2. **Feature interactions:** Automatically finds that education plus occupation matters\n",
    "3. **Adaptive complexity:** Different rules for different data regions\n",
    "4. **Better handling of categorical features:** Can split on multiple categories simultaneously\n",
    "\n",
    "**Confusion Matrix Analysis**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.visualisations import confusion_matrix\n",
    "\n",
    "confusion_matrix.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabedee",
   "metadata": {},
   "source": [
    "**Confusion Matrix Insights**:\n",
    "\n",
    "Once again, we see that LGBM dominates GLM across all metrics of the confusion matrix.\n",
    "- Predicting High Income individuals (72.2% vs 71.5%)\n",
    "- Predicting Low Income individuals (15.3% vs 13.2%)\n",
    "- Predicting High Income for Not High Income individuals (8.1% vs 10.2%)\n",
    "- Predicting Not High Income for High Income individuals (4.8% vs 5.1%)\n",
    "\n",
    "Given LGBM is better at avoiding false positives and false negatives, there is no need to consider which is more important. However, if one model was better at false positives and the other better at false negatives, we would need to consider the business implications to determine which model would be prefered.\n",
    "\n",
    "**Feature Importance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d67cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.visualisations import feature_importance_rank\n",
    "\n",
    "feature_importance_rank.feature_importance_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663924a5",
   "metadata": {},
   "source": [
    "**Feature Importance Insights**\n",
    "\n",
    "Interestingly, the Feature Importance chart shows that the GLM model and LGBM model can rate particualr features very similarly, in the case of capital_net it's no.2 for GLM and no.1 for LGBM, as well as very differently, in the case of relationship = 'Own-child' it's no.1 for GLM and no.33 for LGBM. This highlights the fundamental differences in how these models learn patterns from data. GLM relies on linear relationships and assigns high importance to features with strong individual correlations to the target, whereas LGBM captures complex non-linear interactions and may find a feature redundant if its predictive power is already captured by other variables. The discrepancy with 'Own-child' suggests that while this relationship status has a clear association with income, LGBM likely extracts the same signal through combinations of other features like age and hours worked.\n",
    "\n",
    "**Partial Dependence Analysis**\n",
    "\n",
    "The four Partial Dependence charts below may seem exissive, however, I do think they are incredibly valuable to understanding both how GLM and LGBM models work generally, and the specific insights they highlight for this data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "image_path = src_directory / \"notebooks\" / \"visualisations\" / \"images\"\n",
    "\n",
    "display(Image(filename=image_path / \"partial_dependence_education.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980af751",
   "metadata": {},
   "source": [
    "**Partial Dependence: Education**\n",
    "\n",
    "Education appears to be approximatley linear in both models. It's a good example of not needing a particularly powerful model like LGBM to capture this relationship, a simple regression model like GLM does the job. Furthermore, the story that both models say makes intuitive sense, since it is generally understood that the more education you have the more likley you are to earn a higher paying job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659404dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=image_path / \"partial_dependence_age.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7aa51",
   "metadata": {},
   "source": [
    "**Partial Dependence: Age**\n",
    "\n",
    "Age is a different story. GLM is clearly faily to capture the nuance of the data, which LGBM does capture. Intuitivley, it makes sense that income peaks at around middle-age.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=image_path / \"partial_dependence_capital_net.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12f7bd",
   "metadata": {},
   "source": [
    "**Partial Dependence: Net Capital**\n",
    "\n",
    "Net Capital offers us another interesting story. While GLM predicts a straightforward linear relationship between net capital and high income, an intuitively reasonable assumption, LGBM exhibits peculiar behavior. It assigns very low probability of high income at $3,500 net capital but very high probability at $3,000.\n",
    "\n",
    "This anomaly likely comes from limitations in the data. The vast majority of observations have zero net capital, leaving sparse data for model training at other values. An alternative reason could be a genuine quirk in the 1994 US dataset. Perhaps a policy or event that year resulted in many high earners receiving exactly $3,000 in net capital.\n",
    "\n",
    "Regardless of cause, this example highlights LGBM's remarkable ability to detect patterns at specific values. While this can lead to false patterns when data is limited, you can imagine it being extremely useful for tasks like fraud detection, where fraudsters might consistently use certain, seemingly-random figures that would be hard for humans to notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97733024",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=image_path / \"partial_dependence_relationship.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1ae59",
   "metadata": {},
   "source": [
    "**Partial Dependence: Relationship**\n",
    "\n",
    "Our final story in the Partial Dependence analysis comes from the Relationship field. Unlike the other features show above, this is a categorical feature. LGBM and GLM give approximatley similar values for each of the Relationship categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce970053",
   "metadata": {},
   "source": [
    "## Future Improvements & Business Considerations\n",
    "\n",
    "### Dataset Improvements\n",
    "\n",
    "Although this dataset was valuable in predicting high income, there could be improvements. I list a few below:\n",
    "\n",
    "- Additional features that would likley make our predictions more accurate:\n",
    "    - Geographic granularity\n",
    "        - State/city of residence (not just country)\n",
    "        - Cost of living index for location\n",
    "        - Local unemployment rate\n",
    "    - Detailed education information\n",
    "        - Specific school/university attended\n",
    "        - Subject/s speciallised at university\n",
    "    - Company Information:\n",
    "        - Industry sector employed at\n",
    "        - Company size (small business vs Fortune 500)\n",
    "        - Years at current employer\n",
    "\n",
    "- Target variable\n",
    "    - Binary >$50k or <=$50k was the only income metric avaialble in the dataset\n",
    "    - Although a $50,000 income at the time of the Census was quite a high income, I might have adjusted this thershold slightly has there been data available to do so\n",
    "    - It may have been more business relevant to predict the exact income of an individual, rather than the binary prediction. To do this I would need the dataset to be enhanced with exact incomes of individuals\n",
    "\n",
    "- Cleaner data\n",
    "    - Though I was able to clean the data pretty well, there were some issues that could not be fixed:\n",
    "        - Missing data in occuption and native-country\n",
    "        - Unknown unknowns. It's possible that there were more issues with the data that I failed to find\n",
    "\n",
    "- More data\n",
    "    - Although the sample size was relativley large, it was sparse for some edge cases, like non-zero net capital individuals. As a result, the LGBM model likley overfitted for non-zero net capital individuals\n",
    "    - The data came from a relativley old source, 1994 Census. Additonal data from other census years would provide more confidence in the results being general, rather than 1994 specific\n",
    "\n",
    "- Relevant data\n",
    "    - Most business cases for predicting high income are likley to be interested in predicting high income today, or in the near future\n",
    "        - The most recent Census would be a much stronger data source for achieving this\n",
    "        - Multiple Censuses might help in spotting time trends, to predict future high incomers\n",
    "    - Organisations based in the UK would prefer a data source that originated from people in the UK, not the US\n",
    "\n",
    "### Modelling Improvements\n",
    "\n",
    "#### Other Models\n",
    "\n",
    "Only two models were used in this analysis, however, there is a vast array of other alternatives. For example:\n",
    "\n",
    "- **Alternative for GLM: Generalized Additive Models (GAMs)**\n",
    "    - *What:* This model can be seen as an extension of the GLM model, where we also allow for non-linear functions\n",
    "    - *Why good for this problem:* Allows for non-linear functions for specific features, this is clearly an improvement on our GLM for features like age; still good degree of interpretability due to additivity\n",
    "    - *Downsides:* Overfitting is more of a risk than GLM; Like GLM, we still have to pre-define potential interactions between features\n",
    "\n",
    "- **Alternative for LGBM: XGBoost**\n",
    "    - *What:* Another gradient boosint model, like LGBM; XGBoost uses depth-wise growth, vs LGBM's tree-wise growth\n",
    "    - *Why good for this problem:* Would naturally handle the non-linear age-income relationship and interactions between education/occupation; robust to the outliers in capital_net; can handles the mix of numeric and categorical features\n",
    "    - *Downsides:* Typically less accurate than LGBM on tabular data like this; slower training and larger model size\n",
    "\n",
    "**Alternative vs GLM/LGBM:**\n",
    "\n",
    "- **GAM likley beats GLM:** For our dataset, with a number of non-linear features, it's likely a GAM model would be preffered over our GLM model. I'd recommend trying GAM on this dataset for furture research\n",
    "- **LGBM likley beats XGBoost:** The growth strategy used by LGBM tends to outperform XGBoost's method. Perhaps a more interesting experiment for future research would be a totally differnt style of model, like a neural network, though this dataset's 48k rows might be too small for such a model\n",
    "\n",
    "#### Advanced Feature Engineering\n",
    "\n",
    "With more time I would have liked to delve into some of the following feature engineering techniques.\n",
    "\n",
    "- **Interaction between features**\n",
    "    - Education × Occupation (PhD in healthcare vs PhD in academia)\n",
    "    - Age × Education (early career PhD vs late career PhD)\n",
    "    - Hours_per_week × Occupation (overtime matters differently by job)\n",
    "    - Implementation: Use PolynomialFeatures or manual feature creation\n",
    "\n",
    "- **Polynomial anlysis of features**\n",
    "    - Capital_net transformations (log, sqrt) for skewed distribution\n",
    "    - Could use sklearn's PolynomialFeatures() function\n",
    "\n",
    "- **Binning of features**\n",
    "    - Age groups (young adult, mid-career, pre-retirement)\n",
    "    - Hours worked categories (part-time, full-time, overtime)\n",
    "    - This may help capture non-linear effects more explicitly, especially with GLM\n",
    "\n",
    "#### Hyperparameter Tuning Extensions\n",
    "\n",
    "Sklearn offers many other options for tuning GLM and LGBM models that I did not touch on. Had I, perhaps I would have seen a real improvement in the tuned models vs baseline models.\n",
    "\n",
    "### Critical Business Considerations\n",
    "\n",
    "As mentioned previously, access to more relevant census data would be much more valuable for most organisations interested in this sort of analysis.\n",
    "\n",
    "Moving beyond this limitation, one has to wonder how organisations can deploy such a model. Detailed enough census data is difficult to access (hence why I use 1994 US census). In the real business world I'd expect this model to be a sort of toy model, used as part of the evaluation of whether significant investment towards a model that can be productionalised is worth it.\n",
    "\n",
    "On the other hand, perhaps productionalization of a model isn't required for this model/anlaysis to be deemed a success. It is possible that the insights generated, e.g. feature importance and partial dependence, is where the value lays. For example, a charity may update its potential donor wealth assessment because if this.\n",
    "\n",
    "It's important to keep in mind that the whole reason for data science in business is to generate business value. If an organisation is interested in the insights of a data science project, rather than deploying a model into production, it makes little sense to select a model with high flexibility but low interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
